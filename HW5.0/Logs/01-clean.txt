1

 Processing train books in : ./Raw_books/Train/leblanc
Book  0 written by:  leblanc and has:  877 chunks.
Book  1 written by:  leblanc and has:  775 chunks.
Book  2 written by:  leblanc and has:  1049 chunks.
Book  3 written by:  leblanc and has:  1663 chunks.

 Processing train books in : ./Raw_books/Train/doyle
Book  0 written by:  doyle and has:  683 chunks.
Book  1 written by:  doyle and has:  553 chunks.
Book  2 written by:  doyle and has:  770 chunks.
Book  3 written by:  doyle and has:  1468 chunks.

 Processing train books in : ./Raw_books/Train/christie
Book  0 written by:  christie and has:  75 chunks.
Book  1 written by:  christie and has:  1390 chunks.
Book  2 written by:  christie and has:  903 chunks.
Book  3 written by:  christie and has:  871 chunks.

Done processing, starting tokenizer
Found 24777 unique tokens.
shape of data tensor: (11077, 1000)
shape of label tensor:  (11077,)

 Train/ Val split:  0.8

 Train x shape:  (8861, 1000)

 Val x shape:  (2216, 1000)

Processing test books in : ./Raw_books/Test_UNIVERSE/leblanc
Book  0 written by:  leblanc and has:  1262 chunks.
Book  1 written by:  leblanc and has:  898 chunks.

Processing test books in : ./Raw_books/Test_UNIVERSE/doyle
Book  0 written by:  doyle and has:  587 chunks.
Book  1 written by:  doyle and has:  1101 chunks.

Processing test books in : ./Raw_books/Test_UNIVERSE/christie
Book  0 written by:  christie and has:  1505 chunks.
Book  1 written by:  christie and has:  1125 chunks.

Done processing, starting tokenizer
Test x:  (6478, 1000)
Tesy y:  (6478,)

Processing test books in : ./Raw_books/Test_AUTHOR/leblanc

Processing test books in : ./Raw_books/Test_AUTHOR/doyle
Book  0 written by:  doyle and has:  1379 chunks.
Book  1 written by:  doyle and has:  1116 chunks.

Processing test books in : ./Raw_books/Test_AUTHOR/christie

Done processing, starting tokenizer
Test x:  (2495, 1000)
Tesy y:  (2495,)
